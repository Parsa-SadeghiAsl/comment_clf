{"cells":[{"cell_type":"code","source":["import torch\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from sklearn.model_selection import train_test_split\n","\n","# Paths to positive and negative files\n","pos_file = '/content/drive/MyDrive/DeepLearning/NLP/TextClassifier/rt-polarity-pos.txt'\n","neg_file = '/content/drive/MyDrive/DeepLearning/NLP/TextClassifier/rt-polarity-neg.txt'\n"],"metadata":{"id":"UWciYUSkI0O1","executionInfo":{"status":"ok","timestamp":1679083203793,"user_tz":-210,"elapsed":1047,"user":{"displayName":"Parsa Sadeghi Asl","userId":"10689689238959172922"}}},"id":"UWciYUSkI0O1","execution_count":1,"outputs":[]},{"cell_type":"code","source":["# Load data from files\n","with open(pos_file, 'r', encoding='Windows-1252') as f:\n","    positive_data = f.readlines()\n","with open(neg_file, 'r', encoding='Windows-1252') as f:\n","    negative_data = f.readlines()\n","\n","# Create labels for data\n","positive_labels = [1] * len(positive_data)\n","negative_labels = [0] * len(negative_data)\n"],"metadata":{"id":"QlVljFtBI_Ur","executionInfo":{"status":"ok","timestamp":1679083203796,"user_tz":-210,"elapsed":13,"user":{"displayName":"Parsa Sadeghi Asl","userId":"10689689238959172922"}}},"id":"QlVljFtBI_Ur","execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Load the tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TlLus2WIJ8_e","executionInfo":{"status":"ok","timestamp":1679083205964,"user_tz":-210,"elapsed":2179,"user":{"displayName":"Parsa Sadeghi Asl","userId":"10689689238959172922"}},"outputId":"ea666925-801a-4fa5-bd9e-40b9f4f1ee39"},"id":"TlLus2WIJ8_e","execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["\n","all_data = positive_data + negative_data\n","all_labels = positive_labels + negative_labels\n","\n","train_data, val_data, train_labels, val_labels = train_test_split(\n","    all_data, all_labels, test_size=0.2, random_state=42)\n","\n","# Tokenize the data\n","train_encodings = tokenizer(train_data, truncation=True, padding=True)\n","val_encodings = tokenizer(val_data, truncation=True, padding=True)\n","\n","# Convert the tokenized data into torch tensors\n","train_dataset = torch.utils.data.TensorDataset(\n","    torch.tensor(train_encodings['input_ids']),\n","    torch.tensor(train_encodings['attention_mask']),\n","    torch.tensor(train_labels)\n",")\n","\n","val_dataset = torch.utils.data.TensorDataset(\n","    torch.tensor(val_encodings['input_ids']),\n","    torch.tensor(val_encodings['attention_mask']),\n","    torch.tensor(val_labels)\n",")\n","\n","# Train the model\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n","val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","for epoch in range(3):\n","    train_loss = 0.0\n","    model.train()\n","    for batch in train_loader:\n","        input_ids, attention_mask, labels = batch\n","        input_ids = input_ids.to(device)\n","        attention_mask = attention_mask.to(device)\n","        labels = labels.to(device)\n","        \n","        optimizer.zero_grad()\n","        \n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = criterion(outputs.logits, labels)\n","        loss.backward()\n","        \n","        optimizer.step()\n","        train_loss += loss.item()\n","        \n","    train_loss /= len(train_loader)\n","    \n","    val_loss = 0.0\n","    val_acc = 0.0\n","    model.eval()\n","    with torch.no_grad():\n","        for batch in val_loader:\n","            input_ids, attention_mask, labels = batch\n","            input_ids = input_ids.to(device)\n","            attention_mask = attention_mask.to(device)\n","            labels = labels.to(device)\n","            \n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = criterion(outputs.logits, labels)\n","            val_loss += loss.item()\n","            \n","            preds = outputs.logits.argmax(dim=1)\n","            val_acc += torch.sum(preds == labels).item()\n","    \n","    val_loss /= len(val_loader)\n","    val_acc /= len(val_dataset)\n","    \n","    print(f'Epoch {epoch+1}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, val_acc={val_acc:.4f}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O6gbJ6kmJtGA","executionInfo":{"status":"ok","timestamp":1679083629137,"user_tz":-210,"elapsed":423177,"user":{"displayName":"Parsa Sadeghi Asl","userId":"10689689238959172922"}},"outputId":"1f086c35-f9e2-4e68-8495-535f95394581"},"id":"O6gbJ6kmJtGA","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1: train_loss=0.3976, val_loss=0.3463, val_acc=0.8584\n","Epoch 2: train_loss=0.2260, val_loss=0.3568, val_acc=0.8514\n","Epoch 3: train_loss=0.1264, val_loss=0.4202, val_acc=0.8594\n"]}]}],"metadata":{"kernelspec":{"display_name":"pytorch","language":"python","name":"pytorch"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"colab":{"provenance":[]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}